# ğŸ¤– AI Agents From Scratch

Build AI agents from first principles using Python â€” no heavyweight frameworks, just clean abstractions over the OpenAI API.

This repository walks you through building a fully functional AI agent system step by step, covering everything from basic LLM calls to tool-calling agents with memory, sessions, and structured output. Each chapter introduces a new concept with working code you can run immediately.

## Why This Repo?

Most agent tutorials either hand-wave with pseudocode or throw you into LangChain/CrewAI without explaining what's happening underneath. This repo takes a different approach: you build the primitives yourself â€” the LLM wrapper, tool system, agent loop, memory strategies, and session management â€” so you actually understand what frameworks are doing under the hood.

## Repository Structure

```
ai-agents-from-scratch/
â”œâ”€â”€ chapter_02/                    # Getting started with LLMs
â”‚   â””â”€â”€ getting_started.ipynb      # Basic OpenAI API usage
â”œâ”€â”€ chapter_03/                    # Tools & MCP
â”‚   â”œâ”€â”€ tools.ipynb                # Tool creation and usage
â”‚   â””â”€â”€ custom_tavily_mcp.py       # Custom MCP server with Tavily search
â”œâ”€â”€ chapter_04/                    # Building the agent
â”‚   â”œâ”€â”€ basic_agent.ipynb          # Agent notebook walkthrough
â”‚   â”œâ”€â”€ BaseTool.py                # Abstract tool interface
â”‚   â”œâ”€â”€ FunctionTool.py            # Function-based tool wrapper
â”‚   â”œâ”€â”€ LoadMCPTools.py            # MCP tool loader
â”‚   â”œâ”€â”€ agent_communication.py     # Agent communication patterns
â”‚   â”œâ”€â”€ agent_in_action.py         # Running agents end-to-end
â”‚   â””â”€â”€ gaia_benchmark.py          # GAIA benchmark evaluation
â”œâ”€â”€ chapter_05/                    # Advanced capabilities
â”‚   â”œâ”€â”€ file_system_with_ai_agent.py  # File system agent
â”‚   â”œâ”€â”€ gaia_dataset.py            # GAIA dataset loader
â”‚   â”œâ”€â”€ get_embedding.ipynb        # Embeddings walkthrough
â”‚   â”œâ”€â”€ list_file_tools.py         # File listing tool
â”‚   â”œâ”€â”€ read_file_tool.py          # File reading tool
â”‚   â”œâ”€â”€ read_media_file_tool.py    # Media file reading tool
â”‚   â””â”€â”€ unzip_files_tool.py        # Zip extraction tool
â”œâ”€â”€ scratch_agents/                # Core framework library
â”‚   â”œâ”€â”€ agents/                    # Agent implementations
â”‚   â”‚   â”œâ”€â”€ agent.py               # Simple agent class
â”‚   â”‚   â”œâ”€â”€ agent_result.py        # Agent result container
â”‚   â”‚   â”œâ”€â”€ tool_calling_agent_ch4_base.py       # Base tool-calling agent
â”‚   â”‚   â”œâ”€â”€ tool_calling_agent_ch4_callback.py   # Agent with callbacks
â”‚   â”‚   â”œâ”€â”€ tool_calling_agent_ch4_structured_output.py  # Structured output agent
â”‚   â”‚   â”œâ”€â”€ tool_calling_agent_ch6.py            # Full-featured agent
â”‚   â”‚   â”œâ”€â”€ execution_context_ch4.py  # Ch4 execution context
â”‚   â”‚   â””â”€â”€ execution_context_ch6.py  # Ch6 execution context
â”‚   â”œâ”€â”€ models/                    # LLM abstraction layer
â”‚   â”‚   â”œâ”€â”€ base_llm.py            # Abstract LLM interface
â”‚   â”‚   â”œâ”€â”€ llm_client.py          # LLM client wrapper
â”‚   â”‚   â”œâ”€â”€ llm_communication_layer.py  # Communication layer
â”‚   â”‚   â”œâ”€â”€ llm_request.py         # Request model
â”‚   â”‚   â”œâ”€â”€ llm_response.py        # Response model
â”‚   â”‚   â””â”€â”€ openai.py              # OpenAI implementation
â”‚   â”œâ”€â”€ tools/                     # Tool system
â”‚   â”‚   â”œâ”€â”€ base_tool.py           # Abstract tool with schema generation
â”‚   â”‚   â”œâ”€â”€ function_tool.py       # Wraps plain functions as tools
â”‚   â”‚   â”œâ”€â”€ decorator.py           # @tool decorator
â”‚   â”‚   â”œâ”€â”€ schema_utils.py        # JSON schema utilities
â”‚   â”‚   â”œâ”€â”€ calculator.py          # Calculator tool
â”‚   â”‚   â”œâ”€â”€ search_web.py          # Web search tool
â”‚   â”‚   â”œâ”€â”€ wikipedia.py           # Wikipedia lookup tool
â”‚   â”‚   â”œâ”€â”€ conversation_search.py # Conversation memory search
â”‚   â”‚   â””â”€â”€ core_memory_upsert.py  # Core memory update tool
â”‚   â”œâ”€â”€ memory/                    # Memory strategies
â”‚   â”‚   â”œâ”€â”€ base_memory_strategy.py     # Abstract memory strategy
â”‚   â”‚   â”œâ”€â”€ sliding_window_strategy.py  # Keep last N messages
â”‚   â”‚   â”œâ”€â”€ core_memory_strategy.py     # Persona + user info injection
â”‚   â”‚   â””â”€â”€ summarization_strategy.py   # Summarize older messages
â”‚   â”œâ”€â”€ sessions/                  # Session management
â”‚   â”‚   â”œâ”€â”€ session.py             # Session container (events, state, core memory)
â”‚   â”‚   â”œâ”€â”€ base_session_manager.py      # Session manager interface
â”‚   â”‚   â”œâ”€â”€ in_memory_session_manager.py # In-memory implementation
â”‚   â”‚   â”œâ”€â”€ base_cross_session_manager.py   # Cross-session interface
â”‚   â”‚   â”œâ”€â”€ task_cross_session_manager.py   # Task-based cross-session
â”‚   â”‚   â””â”€â”€ user_cross_session_manager.py   # User-based cross-session
â”‚   â””â”€â”€ types/                     # Core data types
â”‚       â”œâ”€â”€ contents.py            # Message, ToolCall, ToolResult
â”‚       â””â”€â”€ events.py              # Event system
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## Chapters

### Chapter 2 â€” Getting Started
Introduction to calling LLMs with the OpenAI API. Covers basic prompt construction, API configuration, and handling responses.

### Chapter 3 â€” Tools
Learn how to give agents capabilities by defining tools. Build custom tools including a Tavily-powered MCP search server, and understand how tool definitions (JSON schemas) are passed to LLMs for function calling.

### Chapter 4 â€” Building the Agent
The core of the repo. Build a tool-calling agent from scratch with the Think â†’ Act â†’ Observe loop:

1. **Think** â€” send the conversation to the LLM and get a response
2. **Act** â€” execute any tool calls the LLM requests
3. **Observe** â€” feed tool results back and repeat

This chapter progressively builds from a basic agent to ones with callbacks, structured output via a `final_answer` tool pattern, and GAIA benchmark evaluation.

### Chapter 5 â€” Advanced Capabilities
Extend the agent with file system tools (list, read, unzip, media reading), embeddings for semantic search, and dataset evaluation using the GAIA benchmark.

## Core Concepts

### Agent Architecture
```
User Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Agent Loop         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Think (LLM call) â”‚â”€â”€â”¼â”€â”€â†’ If text response â†’ Return to user
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â†“ tool calls   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Act (run tools)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â†“ results      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Observe (feedback) â”‚â”€â”€â”¼â”€â”€â†’ Loop back to Think
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tool System
Tools are defined as classes extending `BaseTool` or as decorated functions using `@tool`. Each tool auto-generates an OpenAI-compatible JSON schema from its Pydantic model or function signature, so the LLM knows how to call it.

### Memory Strategies
Memory is implemented as composable `before_llm_callbacks` that modify the LLM request before it's sent:

- **Sliding Window** â€” keeps only the last N messages to stay within context limits
- **Core Memory** â€” injects persistent persona/user info into every request
- **Summarization** â€” compresses older messages into summaries

### Session Management
Sessions track conversation state, events, and core memory per user. Cross-session managers enable sharing context across conversations (by user or by task).

## Getting Started

### Prerequisites
- Python 3.11+
- An OpenAI API key
- (Optional) Tavily API key for web search, Anthropic API key, HuggingFace token

### Installation

```bash
git clone https://github.com/sajjadanwar0/ai-agents-from-scratch.git
cd ai-agents-from-scratch

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your API keys:
#   OPENAI_API_KEY=sk-...
#   TAVILY_API_KEY=tvly-...    (optional)
#   ANTHROPIC_API_KEY=sk-...   (optional)
#   HF_TOKEN=hf_...            (optional)
```

### Quick Start

Start with the notebooks in order:

```bash
jupyter notebook chapter_02/getting_started.ipynb
```

Or use the agent directly in code:

```python
from scratch_agents.models.openai import OpenAILlm
from scratch_agents.agents.tool_calling_agent_ch4_base import ToolCallingAgent
from scratch_agents.tools.calculator import Calculator
from scratch_agents.tools.search_web import SearchWeb

# Initialize the LLM
model = OpenAILlm(model="gpt-4o-mini")

# Create an agent with tools
agent = ToolCallingAgent(
    name="assistant",
    model=model,
    tools=[Calculator(), SearchWeb()],
    instructions="You are a helpful assistant. Use tools when needed.",
    max_steps=10,
)

# Run the agent
result = await agent.run("What is 42 * 17 and who is the president of France?")
```

## Key Dependencies

| Package | Purpose |
|---------|---------|
| `openai` | LLM API client |
| `pydantic` | Data validation & schema generation |
| `python-dotenv` | Environment variable management |

